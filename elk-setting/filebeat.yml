filebeat.inputs:
  - type: filestream
    id: keyword-logs
    enabled: true
    paths:
      - /usr/share/filebeat/logs/keyword/*.log
    parsers:
      - multiline:
          type: pattern
          pattern: '^\['
          negate: true
          match: after
    fields:
      log_type: keyword
    fields_under_root: true

  - type: filestream
    id: keywordByMember-logs
    enabled: true  # 이 입력을 활성화
    paths:
      - /usr/share/filebeat/logs/keywordByMember/*.log
    parsers:  # 로그 파일을 분석하기 위한 파서 설정입니다.
      - multiline: # 여러 줄로 이어지는 로그를 하나의 이벤트로 처리하는 설정입니다.
          pattern: '^\d{4}/\d{2}/\d{2}'  # 날짜 패턴이 나타나는 줄을 기준으로 합니다.
          negate: true  # 패턴과 일치하지 않는 줄을 계속해서 붙입니다.
          match: after  # 패턴과 일치하는 줄 이후에 오는 줄을 함께 묶습니다.
    fields:
      log_type: keywordByMember  # 로그 타입 필드 추가
    fields_under_root: true  # 추가된 필드를 최상위에 배치

  - type: filestream
    id: status-logs
    enabled: true  # 이 입력을 활성화
    paths:
      - /usr/share/filebeat/logs/status/*.log
    parsers:
      - multiline:
          type: pattern
          pattern: '^\['
          negate: true
          match: after
    fields:
      log_type: status  # 로그 타입 필드 추가
    fields_under_root: true

# 도커 로그 수집을 위해  - /var/run/docker.sock:/var/run/docker.sock 볼륨 설정이 필수.
filebeat.autodiscover:
  providers:
    - type: docker
      hints.enabled: true  # Docker 컨테이너에서 자동으로 로그 수집을 활성화하기 위해 힌트 기반 발견을 사용합니다.
      templates:
        - condition:
          equals:
            docker.container.labels.co.elastic.logs/enabled: "true"
# 위 설정 후,
#     labels:
#      co.elastic.logs/enabled: "true"  # 이 컨테이너의 로그 수집 활성화
#      co.elastic.logs/module: "java"  # Java 모듈을 사용해 로그 처리 (nginx, redis) 등 여러 방식
# 와 같은 것을 도커 컴포즈에는 위 같이, 타 방식은 다른 방식으로 설정해 주면 자동으로


# 로그 메시지가 분석되어 각 필드에 저장되므로 Elasticsearch에서 데이터를 더 세밀하게 분석할 수 있게 됩니다.
# 예를 들어: timestamp나 keyword 필드에 대한 쿼리를 Elasticsearch에서 쉽게 수행할 수 있게 됩니다.

# %{timestamp}: 로그의 타임스탬프가 이 위치에 들어가며, 기본 필드로 추출됩니다.
# %{@timestamp}: 타임스탬프 데이터가 중첩되어 로그로 들어오면 여기서 추출됩니다.
# %{keyword}: Received keyword: 뒤에 나오는 데이터가 keyword 필드로 추출됩니다.

# 수집된 로그를 더 잘 이해할 수 있게 하기 위해 필드를 변환하거나, 데이터를 필터링하거나, 추가적인 정보를 더하는 작업
# 예를 들어 log_type이 "keyword"인 경우:
#2024/09/21 10:15:30 Received keyword: "apple"
#이 로그에서 timestamp와 keyword 필드가 추출되어 Elasticsearch로 전송됩니다.
processors:
  - dissect:
      if: 'ctx.log_type == "keyword"'
      tokenizer: '%{timestamp} Received keyword: %{keyword}'
      field: "message"
      target_prefix: ""

  - dissect:
      if: 'ctx.log_type == "keywordByMember"'
      tokenizer: '%{timestamp} MemberId: %{memberId}, Received keyword: %{keyword}'
      field: "message"
      target_prefix: ""

  - dissect:
      if: 'ctx.log_type == "status"'
      tokenizer: '[%{timestamp}] %{status} - %{method} %{path} %{duration;trim} - %{ip}'
      field: "message"
      target_prefix: ""




  - timestamp:  # 타임스탬프 필드에 대해 처리하는 프로세서입니다.
      field: "timestamp"  # "timestamp" 필드를 기준으로 타임스탬프를 처리합니다.
      layouts:  # 타임스탬프의 형식을 정의합니다.
        - "2006-01-02 15:04:05"  # 로그 타임스탬프 형식을 지정합니다.
      test:  # 테스트용 타임스탬프 예시를 제공합니다.
        - "2024-09-18 11:41:52"
      timezone: "Asia/Seoul"  # 원하는 시간대

  - convert:  # 필드 값 변환을 위한 프로세서입니다.
      fields:  # 변환할 필드 목록입니다.
        - {from: "status", to: "http.response.status_code", type: "integer"}  # "status" 필드를 HTTP 응답 상태 코드로 변환합니다.
        - {from: "duration", to: "event.duration", type: "long"}  # 'duration'을 'long' 타입으로 변환
      ignore_missing: true  # 필드가 없을 경우 무시합니다.
      fail_on_error: false  # 오류가 발생해도 중단하지 않습니다.

  - add_docker_metadata: ~  # Docker 관련 메타데이터를 수집 이벤트에 자동으로 추가합니다.
  - add_host_metadata: ~  # 호스트 시스템에 대한 메타데이터를 추가합니다.

setup.kibana:
  host: ${KIBANA_HOSTS}  # Kibana 호스트 주소를 환경 변수에서 가져옵니다.
  #  Kibana에 접속할 때 사용할 사용자 이름과 비밀번호를 설정합니다.
  #  보통 보안이 활성화된 환경에서 사용합니다.
  username: ${ELASTIC_USER}
  password: ${ELASTIC_PASSWORD}

output.elasticsearch:
  # 내가 임의로 추가
  enabled: true  # Elasticsearch 출력을 활성화합니다. 이 옵션은 일반적으로 필요하지 않으며, 이 설정은 표준 구성 옵션이  아닙니다.
  hosts: ${ELASTIC_HOSTS}  # Elasticsearch 호스트 주소를 환경 변수에서 가져옵니다.
  #  아이디 패스워드 설정시에만 하는거 같음
  username: ${ELASTIC_USER}
  password: ${ELASTIC_PASSWORD}
  # ssl 설정 시에만 하는거 같음 (보안상 매우 권장)
  ssl.enabled: true # 이 설정은 SSL을 사용하여 서버와의 통신을 암호화하겠다는 의미입니다. true로 설정하면, Filebeat는 SSL을 통한 보안 연결을 시도합니다.
  ssl.certificate_authorities: "certs/ca/ca.crt" # 이 경로는 Filebeat가 서버의 SSL 인증서를 신뢰하기 위해 참조하는 인증 기관(CA) 인증서의 위치를 지정합니다. 서버의 인증서가 이 CA에 의해 서명된 경우에만 연결이 허용됩니다.
  ssl.verification_mode: none
  indices:
    - index: "keyword-logs-%{+yyyy.MM.dd}"
      when.equals:
        log_type: "keyword"
    - index: "status-logs-%{+yyyy.MM.dd}"
      when.equals:
        log_type: "status"
    - index: "keywordByMember-logs-%{+yyyy.MM.dd}"
      when.equals:
        log_type: "keywordByMember"
# Filebeat에서 바로 completion 타입을 할당할 수는 없고, Elasticsearch의 인덱스 템플릿을 사용하여 특정 필드에 대해 completion 타입을 설정해야 합니다.
# 기본적으로는 text이라는 타입이 할당
# "type": "text"

setup.template:
  # 하나의 템플릿으로 여러 패턴에 적용
  - name: "specific-logs"
    pattern: "keyword-logs-*,status-logs-*,keywordByMember-logs-*"
  # keyword-logs 템플릿
  - name: "keyword-logs-template"
    pattern: "keyword-logs-*"
  # status-logs 템플릿
  - name: "status-logs-template"
    pattern: "status-logs-*"
  # keywordByMember-logs 템플릿
  - name: "keywordByMember-logs-template"
    pattern: "keywordByMember-logs-*"
#    settings:
#      index:
#        number_of_shards: 1
#        number_of_replicas: 1



logging.level: debug
logging.to_files: true
logging.files:
  path: /var/log/filebeat
  name: filebeat
  keepfiles: 7
  permissions: 0644

# 이 기본 정책은 일반적으로 다음과 같은 단계를 포함합니다:
#Hot 단계: 새 데이터가 쓰이는 단계
#Delete 단계: 설정된 기간 후 데이터를 삭제하는 단계
#setup.ilm.enabled: false
#setup.ilm.rollover_alias: "filebeat"
#setup.ilm.pattern: "{now/d}-000001"

# 아래 로그
# http.enabled: true
# http.host: 0.0.0.0
# http.port: 5066

#queue.type: persisted
#queue.mem.events: 4096
#queue.disk.path: /var/lib/filebeat/queue
#
#max_procs: 4

# 이 설정은 Docker 컨테이너에서 발생하는 로그를 동시에 수집하여 Elasticsearch로 전송합니다.
# 수집된 로그에는 Docker 메타데이터가 추가되며, Kibana를 통해 로그 데이터를 시각화할 수 있습니다.
# 또한, 이 설정은 환경 변수에서 Elasticsearch 및 Kibana의 주소를 가져와 사용하기 때문에, 유연하게 환경 설정을 변경할 수 있습니다.


#위의 예시에서는 현재 로그를 전송하려는 서버의 환경 정보와 프로젝트 명을 추가하여 전달하도록 하였다.
#그 결과 아래와 같이 fields.env와 fields.project 필드가 추가된 것을 확인할 수 있다.

#  위의 로그에서 v2, restUri... 로 시작하는 부분은 위의 2021-11-16 ... RestApiBO와 하나의 로그로 보이는데, 로그 출력 시 에 \n으로 개행 처리가 되어 있어 별도의 로그로 전송된 것으로 보인다.
#  우리는 2021-11-16 10:59:07 [ 과 같은 패턴을 구분자로 하여 여러 라인의 메세지를 1개의 메세지로 인식하도록 처리해야 한 다. 이에 대한 pattern을 분석해보니 다음과 같다.
#
#  숫자4개-숫자2개-숫자2개 숫자2개:숫자2개:숫자2개 [
#
#  그래서 이러한 패턴을 multiline.pattern으로 추가하고, multiline.negate는 true, multiline.match는 after로 주면 되며 이 는 같은 패턴이 나올때까지 이전 로그에 붙인다는 것을 의미한다.

# ---

#filebeat.inputs:
#  - type: log
#    enabled: true  # 이 입력을 활성화합니다. false일 경우 이 입력 설정은 무시됩니다.
#
#    paths:
#      # 로그 파일이 위치한 경로를 지정합니다. 예를 들어 /home/mangkyu/logs/tomcat/mangkyu-elk/*.log 같이 설정할 수 있습니다.
#      - /home/로그경로...
#
#    # 필요한 경우 추가적인 필드를 정의할 수 있습니다. 이 필드들은 Elasticsearch에 저장될 때 각 로그에 추가됩니다.
#    fields:
#      env: "real"  # 환경 설정값, 예를 들어 'real'은 실제 운영 환경을 의미합니다.
#      project: "openapi"  # 프로젝트 이름을 지정합니다.
#
#    # 여러 줄로 구성된 로그를 하나의 이벤트로 처리하기 위한 설정입니다.
#    multiline.pattern: ^[0-9]{4}-[0-9]{2}-[0-9]{2}[[:space:]][0-9]{2}:[0-9]{2}:[0-9]{2}[[:space:]]\[
#    multiline.negate: true  # 패턴이 일치하지 않는 행에서 새로운 로그 이벤트가 시작됩니다.
#    multiline.match: after  # 일치하는 패턴 다음 줄부터 현재 이벤트에 포함시킵니다.
#


# Docker 모듈도 있으며, 이를 활성화하여 사용하면 됩니다:
#filebeat.modules:
#  - module: docker
#    log:
#      input:
#        enabled: true
#        paths:
#          - /var/lib/docker/containers/*/*.log